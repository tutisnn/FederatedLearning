{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T15:45:46.494186Z",
     "iopub.status.busy": "2025-10-26T15:45:46.493890Z",
     "iopub.status.idle": "2025-10-26T15:45:56.974148Z",
     "shell.execute_reply": "2025-10-26T15:45:56.973340Z",
     "shell.execute_reply.started": "2025-10-26T15:45:46.494164Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: trl in /usr/local/lib/python3.11/dist-packages (0.24.0)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.48.1)\n",
      "Requirement already satisfied: flwr in /usr/local/lib/python3.11/dist-packages (1.22.0)\n",
      "Collecting flwr_datasets\n",
      "  Downloading flwr_datasets-0.5.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: accelerate>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from trl) (1.9.0)\n",
      "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from trl) (4.1.1)\n",
      "Requirement already satisfied: transformers>=4.56.1 in /usr/local/lib/python3.11/dist-packages (from trl) (4.57.1)\n",
      "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (25.0)\n",
      "Requirement already satisfied: click<8.2.0 in /usr/local/lib/python3.11/dist-packages (from flwr) (8.1.8)\n",
      "Requirement already satisfied: cryptography<45.0.0,>=44.0.1 in /usr/local/lib/python3.11/dist-packages (from flwr) (44.0.3)\n",
      "Requirement already satisfied: grpcio!=1.65.0,<2.0.0,>=1.62.3 in /usr/local/lib/python3.11/dist-packages (from flwr) (1.75.1)\n",
      "Requirement already satisfied: grpcio-health-checking<2.0.0,>=1.62.3 in /usr/local/lib/python3.11/dist-packages (from flwr) (1.62.3)\n",
      "Requirement already satisfied: iterators<0.0.3,>=0.0.2 in /usr/local/lib/python3.11/dist-packages (from flwr) (0.0.2)\n",
      "Requirement already satisfied: pathspec<0.13.0,>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from flwr) (0.12.1)\n",
      "Requirement already satisfied: protobuf<5.0.0,>=4.21.6 in /usr/local/lib/python3.11/dist-packages (from flwr) (4.25.8)\n",
      "Requirement already satisfied: pycryptodome<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from flwr) (3.23.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /usr/local/lib/python3.11/dist-packages (from flwr) (6.0.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from flwr) (2.32.5)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.5.0 in /usr/local/lib/python3.11/dist-packages (from flwr) (13.9.4)\n",
      "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from flwr) (2.3.0)\n",
      "Requirement already satisfied: tomli-w<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from flwr) (1.2.0)\n",
      "Requirement already satisfied: typer<0.13.0,>=0.12.5 in /usr/local/lib/python3.11/dist-packages (from flwr) (0.12.5)\n",
      "Collecting datasets>=3.0.0 (from trl)\n",
      "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting matplotlib<4.0.0,>=3.7.5 (from flwr_datasets)\n",
      "  Downloading matplotlib-3.10.7-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting seaborn<0.14.0,>=0.13.0 (from flwr_datasets)\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from flwr_datasets) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (7.1.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.4.0->trl) (0.5.3)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography<45.0.0,>=44.0.1->flwr) (2.0.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (3.19.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (22.0.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets>=3.0.0->trl)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
      "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (3.12.15)\n",
      "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.11/dist-packages (from grpcio!=1.65.0,<2.0.0,>=1.62.3->flwr) (4.15.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.5->flwr_datasets) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.5->flwr_datasets) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.5->flwr_datasets) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.5->flwr_datasets) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.5->flwr_datasets) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.5->flwr_datasets) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.5->flwr_datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr) (2025.8.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0.0,>=13.5.0->flwr) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0.0,>=13.5.0->flwr) (2.19.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.56.1->trl) (2025.9.18)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.56.1->trl) (0.22.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->flwr) (1.5.4)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography<45.0.0,>=44.0.1->flwr) (2.23)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl) (1.20.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (1.1.10)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.5.0->flwr) (0.1.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib<4.0.0,>=3.7.5->flwr_datasets) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
      "Downloading flwr_datasets-0.5.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.0/87.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.10.7-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fsspec, dill, matplotlib, seaborn, datasets, flwr_datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.9.0\n",
      "    Uninstalling fsspec-2025.9.0:\n",
      "      Successfully uninstalled fsspec-2025.9.0\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.4.0\n",
      "    Uninstalling dill-0.4.0:\n",
      "      Successfully uninstalled dill-0.4.0\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.7.2\n",
      "    Uninstalling matplotlib-3.7.2:\n",
      "      Successfully uninstalled matplotlib-3.7.2\n",
      "  Attempting uninstall: seaborn\n",
      "    Found existing installation: seaborn 0.12.2\n",
      "    Uninstalling seaborn-0.12.2:\n",
      "      Successfully uninstalled seaborn-0.12.2\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 4.1.1\n",
      "    Uninstalling datasets-4.1.1:\n",
      "      Successfully uninstalled datasets-4.1.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "cudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "ydata-profiling 4.17.0 requires matplotlib<=3.10,>=3.5, but you have matplotlib 3.10.7 which is incompatible.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.9.0 which is incompatible.\n",
      "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 flwr_datasets-0.5.0 fsspec-2024.9.0 matplotlib-3.10.7 seaborn-0.13.2\n"
     ]
    }
   ],
   "source": [
    "!pip install trl  bitsandbytes flwr flwr_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T15:28:57.445782Z",
     "iopub.status.busy": "2025-10-26T15:28:57.444901Z",
     "iopub.status.idle": "2025-10-26T15:29:03.159921Z",
     "shell.execute_reply": "2025-10-26T15:29:03.159248Z",
     "shell.execute_reply.started": "2025-10-26T15:28:57.445754Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/notebook/utils.py:280: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  return LooseVersion(v) >= LooseVersion(check)\n",
      "/usr/local/lib/python3.11/dist-packages/google/colab/_import_hooks/_pydrive.py:21: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "  import imp  # pylint: disable=deprecated-module\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtuhanasinan\u001b[0m (\u001b[33mtuhanasinan-t-c-i-stanbul-sabahattin-zaim-niversitesi\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"YOUR-API-KEY\")\n",
    "import wandb\n",
    "\n",
    "wandb.login(key=\"your-api-key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-26T15:29:07.376431Z",
     "iopub.status.busy": "2025-10-26T15:29:07.375370Z",
     "iopub.status.idle": "2025-10-26T15:29:07.386144Z",
     "shell.execute_reply": "2025-10-26T15:29:07.385187Z",
     "shell.execute_reply.started": "2025-10-26T15:29:07.376403Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf, DictConfig\n",
    "\n",
    "cfg_dict = {\n",
    "    'dataset': {\n",
    "        'name': 'medalpaca/medical_meadow_medical_flashcards'\n",
    "    },\n",
    "    'model': {\n",
    "        'name': \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "        'quantization': 4,\n",
    "        'gradient_checkpointing': True,\n",
    "        'use_fast_tokenizer': True,\n",
    "        'lora': {\n",
    "            'peft_lora_r': 16,\n",
    "            'peft_lora_alpha': 64,\n",
    "            'target_modules': None}},\n",
    "    'train': {\n",
    "        'num_rounds': '${flower.num_rounds}',\n",
    "        'save_every_round': 5,\n",
    "        'learning_rate_max': 5e-05,\n",
    "        'learning_rate_min': 1e-06,\n",
    "        'seq_length': 1024,\n",
    "        'padding_side': 'left',\n",
    "        'evaluate_split': True,\n",
    "        'training_arguments': {\n",
    "            'output_dir': None,\n",
    "            'learning_rate': None,\n",
    "            'per_device_train_batch_size': 2,\n",
    "            'gradient_accumulation_steps': 1,\n",
    "            'logging_steps': 5,\n",
    "            'max_steps': 250,\n",
    "            # 'num_train_epochs': 1,\n",
    "            'report_to': 'wandb',\n",
    "            'run_name': \"llama32_1b_fed_sft_take2\",\n",
    "            'save_steps': 1000,\n",
    "            'save_total_limit': 10,\n",
    "            'gradient_checkpointing': '${model.gradient_checkpointing}',\n",
    "            'lr_scheduler_type': 'constant'}},\n",
    "    'flower': {\n",
    "        'num_clients': 20,\n",
    "        'num_rounds': 2,\n",
    "        'fraction_fit': 0.2,\n",
    "        'client_resources': {\n",
    "            'num_cpus': 2,\n",
    "            'num_gpus': 1.0\n",
    "        },\n",
    "        'dp': {\n",
    "            'noise_mult': 0.02,\n",
    "            'clip_norm': 0.5\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "cfg = OmegaConf.create(cfg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T15:29:10.636531Z",
     "iopub.status.busy": "2025-10-26T15:29:10.635564Z",
     "iopub.status.idle": "2025-10-26T15:29:17.678095Z",
     "shell.execute_reply": "2025-10-26T15:29:17.677325Z",
     "shell.execute_reply.started": "2025-10-26T15:29:10.636490Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model:\n",
      "\t1237.518 M parameters\n",
      "\t4720.76 MB --> upload in 1888.30s @ 20Mbps\n",
      "Finetuned model:\n",
      "\t1.704 M parameters\n",
      "\t6.50 MB --> upload in 2.60s @ 20Mbps\n",
      "Federated Learning setting: \n",
      "\tNumber of rounds: 2\n",
      "\tNumber of clients per round: 4\n",
      "-----------------------------------------------\n",
      "Total Communication costs (Full model): 73.8 GB\n",
      "Total Communication costs (Finetuning): 104.0 MB\n",
      "Communication savings: 726.3x\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments  # SFTConfig için eklendi\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "from peft.utils import prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig  # <-- Burası düzeltildi\n",
    "\n",
    "def get_model(model_cfg: DictConfig):\n",
    "    \"\"\"Load model with appropiate quantization config and\n",
    "    other optimizations. Notice here that we are returning the\n",
    "    LoRA model, not the full model.\"\"\"\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    quantization_config = None\n",
    "    model_name = model_cfg.name\n",
    "    if use_cuda:\n",
    "        if model_cfg.quantization == 4:\n",
    "            quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "        elif model_cfg.quantization == 8:\n",
    "            quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Use 4-bit or 8-bit quantization. You passed: {model_cfg.quantization}/\"\n",
    "            )\n",
    "\n",
    "        model_name = model_cfg.name\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quantization_config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "\n",
    "    if use_cuda:\n",
    "        model = prepare_model_for_kbit_training(\n",
    "            model, use_gradient_checkpointing=model_cfg.gradient_checkpointing\n",
    "        )\n",
    "\n",
    "    target_modules = model_cfg.lora.target_modules\n",
    "    if target_modules:\n",
    "        target_modules = list(target_modules)\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        r=model_cfg.lora.peft_lora_r,\n",
    "        lora_alpha=model_cfg.lora.peft_lora_alpha,\n",
    "        lora_dropout=0.075,\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=target_modules,\n",
    "    )\n",
    "\n",
    "    peft_model = get_peft_model(model, peft_config)\n",
    "    if not (use_cuda):\n",
    "        peft_model.enable_input_require_grads()\n",
    "\n",
    "    if model_cfg.gradient_checkpointing:\n",
    "        model.config.use_cache = False\n",
    "\n",
    "    return peft_model\n",
    "\n",
    "def compute_communication_costs(config, comm_bw_mbps: float = 20):\n",
    "      \"\"\"\n",
    "      We use Flower's helper function to compute communication costs of federated finetuning.\n",
    "      Get ready to see the savings!\n",
    "      \"\"\"\n",
    "      model = get_model(config.model)\n",
    "\n",
    "      trainable, all_parameters = model.get_nb_trainable_parameters()\n",
    "\n",
    "      total_size = 4*all_parameters/(1024**2)\n",
    "      trainable_size = 4*trainable/(1024**2)\n",
    "\n",
    "      upload_time_total = total_size/(comm_bw_mbps/8) # Convert Mbps to MBps by dividing by 8\n",
    "      upload_time_finetune = trainable_size/(comm_bw_mbps/8)\n",
    "\n",
    "      print(f\"Full model:\\n\\t{all_parameters/1e6:.3f} M parameters\\n\\t{total_size:.2f} MB --> upload in {upload_time_total:.2f}s @ {comm_bw_mbps}Mbps\")\n",
    "      print(f\"Finetuned model:\\n\\t{trainable/1e6:.3f} M parameters\\n\\t{trainable_size:.2f} MB --> upload in {upload_time_finetune:.2f}s @ {comm_bw_mbps}Mbps\")\n",
    "      # print(f\"In a {comm_bw_mbps} Mbps channel --> {}\")\n",
    "\n",
    "      num_rounds = config.flower.num_rounds\n",
    "      num_clients_per_round = int(config.flower.num_clients * config.flower.fraction_fit)\n",
    "      print(f\"Federated Learning setting: \"\n",
    "            f\"\\n\\tNumber of rounds: {num_rounds}\"\n",
    "            f\"\\n\\tNumber of clients per round: {num_clients_per_round}\")\n",
    "\n",
    "      print(f\"-----------------------------------------------\")\n",
    "      print(f\"Total Communication costs (Full model): {2*num_rounds*num_clients_per_round*total_size/1024:.1f} GB\")\n",
    "      print(f\"Total Communication costs (Finetuning): {2*num_rounds*num_clients_per_round*trainable_size} MB\")\n",
    "      print(f\"Communication savings: {all_parameters/trainable:.1f}x\")\n",
    "\n",
    "compute_communication_costs(cfg, comm_bw_mbps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T16:32:55.763427Z",
     "iopub.status.busy": "2025-10-26T16:32:55.763054Z",
     "iopub.status.idle": "2025-10-26T16:32:56.495963Z",
     "shell.execute_reply": "2025-10-26T16:32:56.495255Z",
     "shell.execute_reply.started": "2025-10-26T16:32:55.763402Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tokenizer, data_collator (None), ve formatting_prompts_func tanımlandı (Doğru Versiyon).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer # Gerekli import'u en üste taşıdım\n",
    "from omegaconf import DictConfig # cfg için gerekli\n",
    "\n",
    "def formatting_prompts_func(example):\n",
    "    \"\"\"\n",
    "    Constructing a standard Alpaca prompt for a SINGLE example.\n",
    "    'example' is a dictionary (a single row), not a batch.\n",
    "    \"\"\"\n",
    "    mssg = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    text = f\"{mssg}\\n### Instruction:\\n{example['instruction']}\\n### Response: {example['response']}\"\n",
    "    return text\n",
    "\n",
    "def get_tokenizer_and_data_collator_and_prompt_formatting(\n",
    "    model_name: str, use_fast: bool, padding_side: str\n",
    "):\n",
    "    \"\"\"Loads tokenizer, sets padding, returns None for collator.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name, use_fast=use_fast, padding_side=padding_side\n",
    "    )\n",
    "    tokenizer.pad_token = (\n",
    "        tokenizer.bos_token if padding_side == \"left\" else tokenizer.eos_token\n",
    "    )\n",
    "    # TRL 0.24.0 için DataCollator'ı None olarak ayarlamak doğrudur.\n",
    "    data_collator = None\n",
    "    # Düzeltilmiş (doğru) fonksiyonu döndürür\n",
    "    return tokenizer, data_collator, formatting_prompts_func\n",
    "\n",
    "# Fonksiyonu SADECE BİR KERE çağırın\n",
    "tokenizer, data_collator, formatting_prompts_func = get_tokenizer_and_data_collator_and_prompt_formatting(\n",
    "    cfg.model.name,\n",
    "    cfg.model.use_fast_tokenizer,\n",
    "    cfg.train.padding_side,\n",
    ")\n",
    "\n",
    "print(\"✅ Tokenizer, data_collator (None), ve formatting_prompts_func tanımlandı (Doğru Versiyon).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T15:30:00.647919Z",
     "iopub.status.busy": "2025-10-26T15:30:00.647289Z",
     "iopub.status.idle": "2025-10-26T15:30:00.652280Z",
     "shell.execute_reply": "2025-10-26T15:30:00.651516Z",
     "shell.execute_reply.started": "2025-10-26T15:30:00.647892Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def cosine_annealing(\n",
    "    current_round: int,\n",
    "    total_round: int,\n",
    "    lrate_max: float = 0.001,\n",
    "    lrate_min: float = 0.0,\n",
    ") -> float:\n",
    "    \"\"\"Implement cosine annealing learning rate schedule. Strictly speaking this\n",
    "    is not necessary.\"\"\"\n",
    "\n",
    "    cos_inner = math.pi * current_round / total_round\n",
    "    return lrate_min + 0.5 * (lrate_max - lrate_min) * (1 + math.cos(cos_inner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T16:35:28.929473Z",
     "iopub.status.busy": "2025-10-26T16:35:28.928848Z",
     "iopub.status.idle": "2025-10-26T16:35:28.944278Z",
     "shell.execute_reply": "2025-10-26T16:35:28.943450Z",
     "shell.execute_reply.started": "2025-10-26T16:35:28.929447Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from flwr.common import Context\n",
    "from flwr.common.typing import NDArrays, Scalar\n",
    "from flwr.client import NumPyClient\n",
    "from typing import Dict, Tuple, Callable\n",
    "from collections import OrderedDict\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from omegaconf import DictConfig, OmegaConf # Gerekli import eklendi\n",
    "from peft import get_peft_model_state_dict, set_peft_model_state_dict\n",
    "# Not: get_model, cosine_annealing ve set_parameters fonksiyonlarınızın\n",
    "# bu kod bloğunun dışında bir yerde zaten tanımlı olduğunu varsayıyorum.\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "#                           DÜZELTİLMİŞ KOD\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "class FlowerClient(NumPyClient):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_cfg: DictConfig,\n",
    "        train_cfg: DictConfig,\n",
    "        trainset,\n",
    "        tokenizer,\n",
    "        formatting_prompts_func,\n",
    "        data_collator, \n",
    "        save_path,\n",
    "    ):\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.train_cfg = train_cfg\n",
    "        \n",
    "        # --- DÜZELTME 1: SFTConfig 'completion_only_loss' ve 'dataset_text_field' için güncellendi ---\n",
    "        sft_config_dict = OmegaConf.to_container(train_cfg.training_arguments, resolve=True)\n",
    "        \n",
    "        self.training_arguments = SFTConfig(\n",
    "            **sft_config_dict,\n",
    "            completion_only_loss=True,  # Sadece yanıtı eğit\n",
    "            dataset_text_field=\"text\"   # Formatlanmış verinin sütun adı\n",
    "        )\n",
    "        # --- DÜZELTME 1 SONU ---\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.formatting_prompts_func = formatting_prompts_func\n",
    "        self.data_collator = data_collator\n",
    "        self.save_path = save_path\n",
    "        self.model = get_model(model_cfg)\n",
    "        self.trainset = trainset\n",
    "\n",
    "    def get_parameters(self, config: Dict[str, Scalar]) -> NDArrays:\n",
    "        \"\"\"Return the parameters of the current net.\"\"\"\n",
    "        state_dict = get_peft_model_state_dict(self.model)\n",
    "        return [val.cpu().numpy() for _, val in state_dict.items()]\n",
    "\n",
    "    def fit(\n",
    "        self, parameters: NDArrays, config: Dict[str, Scalar]\n",
    "    ) -> Tuple[NDArrays, int, Dict]:\n",
    "        \"\"\"Implement distributed fit function for a given client.\"\"\"\n",
    "        set_parameters(self.model, parameters)\n",
    "\n",
    "        new_lr = cosine_annealing(\n",
    "            int(config[\"current_round\"]),\n",
    "            self.train_cfg.num_rounds,\n",
    "            self.train_cfg.learning_rate_max,\n",
    "            self.train_cfg.learning_rate_min,\n",
    "        )\n",
    "\n",
    "        self.training_arguments.learning_rate = new_lr\n",
    "        self.training_arguments.output_dir = self.save_path\n",
    "\n",
    "        evalset = None\n",
    "        if self.train_cfg.evaluate_split:\n",
    "            train_test = self.trainset.train_test_split(test_size=0.1, seed=1234)\n",
    "            trainset = train_test['train']\n",
    "            evalset = train_test['test']\n",
    "        else:\n",
    "            trainset = self.trainset\n",
    "\n",
    "        # --- DÜZELTME 2: Formatlama SFTTrainer'dan ÖNCE yapılır ---\n",
    "        \n",
    "        # Veri setini formatlamak için (doğru 'formatting_prompts_func' kullanılır)\n",
    "        def format_dataset(example):\n",
    "            return {\"text\": self.formatting_prompts_func(example)}\n",
    "\n",
    "        # .map() ile formatlamayı uygula\n",
    "        trainset = trainset.map(format_dataset, batched=False)\n",
    "        if evalset:\n",
    "            evalset = evalset.map(format_dataset, batched=False)\n",
    "        # --- DÜZELTME 2.A SONU ---\n",
    "\n",
    "        trainer = SFTTrainer(\n",
    "            model=self.model,\n",
    "            processing_class=self.tokenizer, # 'processing_class' yerine 'tokenizer' kullanılır\n",
    "            args=self.training_arguments, \n",
    "            train_dataset=trainset,       \n",
    "            eval_dataset=evalset,\n",
    "            # 'formatting_func' kaldırıldı (çünkü önceden mapledik)\n",
    "            data_collator=self.data_collator,\n",
    "        )\n",
    "        # --- DÜZELTME 2.B SONU ---\n",
    "\n",
    "        metrics = {}\n",
    "        if self.train_cfg.evaluate_split:\n",
    "            eval_res = trainer.evaluate()\n",
    "            metrics['eval_loss'] = eval_res['eval_loss']\n",
    "            print(eval_res)\n",
    "\n",
    "        results = trainer.train()\n",
    "        metrics = {**metrics, \"train_loss\": results.training_loss}\n",
    "\n",
    "        return (\n",
    "            self.get_parameters({}),\n",
    "            len(self.trainset),\n",
    "            metrics,\n",
    "        )\n",
    "\n",
    "def gen_client_fn(\n",
    "    fds,\n",
    "    tokenizer,\n",
    "    formatting_prompts_func,\n",
    "    data_collator,\n",
    "    model_cfg: DictConfig,\n",
    "    train_cfg: DictConfig,\n",
    "    save_path: str,\n",
    ") -> Callable[[str], FlowerClient]:\n",
    "    \"\"\"Generate the client function that creates the Flower Clients.\"\"\"\n",
    "\n",
    "    def client_fn(context: Context) -> FlowerClient:\n",
    "        \"\"\"Create a Flower client representing a single organization.\"\"\"\n",
    "\n",
    "        partition_id = int(context.node_config[\"partition-id\"])\n",
    "        client_trainset = fds.load_partition(partition_id, \"train\")\n",
    "\n",
    "        # --- DÜZELTME BURADA ---\n",
    "        # Veri setinde zaten 'instruction' var. 'input'u silelim.\n",
    "        # 'output'u 'response' yapalım.\n",
    "        client_trainset = client_trainset.remove_columns([\"input\"])\n",
    "        client_trainset = client_trainset.rename_column(\"output\", \"response\")\n",
    "        # --- DÜZELTME SONU ---\n",
    "        \n",
    "        # Artık sütunlar: ['output', 'instruction'] -> ['response', 'instruction']\n",
    "        # formatting_prompts_func bunları kullanabilir.\n",
    "        return FlowerClient(\n",
    "            model_cfg,\n",
    "            train_cfg,\n",
    "            client_trainset,\n",
    "            tokenizer,\n",
    "            formatting_prompts_func,\n",
    "            data_collator,\n",
    "            save_path,\n",
    "        ).to_client()\n",
    "\n",
    "    return client_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T16:35:34.132789Z",
     "iopub.status.busy": "2025-10-26T16:35:34.132152Z",
     "iopub.status.idle": "2025-10-26T16:35:34.138377Z",
     "shell.execute_reply": "2025-10-26T16:35:34.137453Z",
     "shell.execute_reply.started": "2025-10-26T16:35:34.132762Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/flwr_datasets/utils.py:109: UserWarning: The currently tested dataset are ['mnist', 'ylecun/mnist', 'cifar10', 'uoft-cs/cifar10', 'fashion_mnist', 'zalando-datasets/fashion_mnist', 'sasha/dog-food', 'zh-plus/tiny-imagenet', 'scikit-learn/adult-census-income', 'cifar100', 'uoft-cs/cifar100', 'svhn', 'ufldl-stanford/svhn', 'sentiment140', 'stanfordnlp/sentiment140', 'speech_commands', 'LIUM/tedlium', 'flwrlabs/femnist', 'flwrlabs/ucf101', 'flwrlabs/ambient-acoustic-context', 'jlh/uci-mushrooms', 'Mike0307/MNIST-M', 'flwrlabs/usps', 'scikit-learn/iris', 'flwrlabs/pacs', 'flwrlabs/cinic10', 'flwrlabs/caltech101', 'flwrlabs/office-home', 'flwrlabs/fed-isic2019']. Given: medalpaca/medical_meadow_medical_flashcards.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from flwr.client.mod import fixedclipping_mod\n",
    "import flwr as fl\n",
    "from flwr_datasets import FederatedDataset\n",
    "save_path = \"./my_fl_llama_model\"\n",
    "fds = FederatedDataset(\n",
    "    dataset=cfg.dataset.name, \n",
    "    partitioners={\"train\": cfg.flower.num_clients}\n",
    ")\n",
    "\n",
    "client = fl.client.ClientApp(\n",
    "    client_fn=gen_client_fn(\n",
    "        fds,\n",
    "        tokenizer,\n",
    "        formatting_prompts_func,\n",
    "        data_collator,\n",
    "        cfg.model,\n",
    "        cfg.train,\n",
    "        save_path,\n",
    "    ),\n",
    "    mods=[fixedclipping_mod]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T16:35:37.008778Z",
     "iopub.status.busy": "2025-10-26T16:35:37.007945Z",
     "iopub.status.idle": "2025-10-26T16:35:37.016029Z",
     "shell.execute_reply": "2025-10-26T16:35:37.015123Z",
     "shell.execute_reply.started": "2025-10-26T16:35:37.008749Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from flwr.common import Context\n",
    "\n",
    "# All docstrings are written by me\n",
    "\n",
    "def get_on_fit_config():\n",
    "    \"\"\"\n",
    "     • Purpose: This function provides the configuration dictionary sent to each\n",
    "      client before training begins in a given round.\n",
    "     • Use Case: Clients can adapt behaviors (e.g., learning rate schedules) based\n",
    "        on the current round number.\n",
    "     • Design Pattern: Returns a function (fit_config_fn) that Flower calls at each\n",
    "      round.\n",
    "\n",
    "    🧠 Analogy: This is like giving each chef a new cooking instruction every day\n",
    "    based on how many days the kitchen has been operating.\n",
    "\n",
    "    \"\"\"\n",
    "    def fit_config_fn(server_round: int):\n",
    "        fit_config = {\"current_round\": server_round}\n",
    "        return fit_config\n",
    "\n",
    "    return fit_config_fn\n",
    "\n",
    "def fit_weighted_average(metrics):\n",
    "    \"\"\"\n",
    "    • Purpose: Calculates a weighted average of training loss across all clients.\n",
    "    • Mechanism:\n",
    "        • Each client’s training loss is scaled by the number of examples it used.\n",
    "        • This ensures larger datasets have more influence on the final average.\n",
    "    • Why It Matters: Without weighting, small datasets could skew the overall metric.\n",
    "\n",
    "    Example:\n",
    "    # Client 1: 100 examples, loss = 0.5 → 100 * 0.5 = 50\n",
    "    # Client 2: 200 examples, loss = 0.25 → 200 * 0.25 = 50\n",
    "    # Weighted average = (50 + 50) / (100 + 200) = 0.333\n",
    "    \"\"\"\n",
    "    # Multiply accuracy of each client by number of examples used\n",
    "    losses = [num_examples * m[\"train_loss\"] for num_examples, m in metrics]\n",
    "    examples = [num_examples for num_examples, _ in metrics]\n",
    "\n",
    "    # Aggregate and return custom metric (weighted average)\n",
    "    return {\"train_loss\": sum(losses) / sum(examples)}\n",
    "\n",
    "def get_evaluate_fn(model_cfg, save_every_round, total_round, save_path):\n",
    "    \"\"\"\n",
    "     • Purpose: Save the global model periodically during training.\n",
    "     • Conditions:\n",
    "       • Skip round 0.\n",
    "       • Save if it’s the final round or every n rounds (save_every_round).\n",
    "       • How:\n",
    "         • Reconstruct the model from the config.\n",
    "         • Load the current global parameters.\n",
    "         • Save using the HuggingFace save_pretrained method.\n",
    "\n",
    "    🧠 Why This Matters: In federated learning, there’s no single centralized\n",
    "    training process. If something goes wrong, saved checkpoints are your recovery\n",
    "    point.\n",
    "\n",
    "    🔁 Return Format: Always returns 0.0, {} because evaluation is optional here\n",
    "    — the function is used mainly for checkpointing.\n",
    "    \"\"\"\n",
    "\n",
    "    def evaluate(server_round: int, parameters, config):\n",
    "        # Save model\n",
    "        if server_round != 0 and (\n",
    "            server_round == total_round or server_round % save_every_round == 0\n",
    "        ):\n",
    "            # Init model\n",
    "            model = get_model(model_cfg)\n",
    "            set_parameters(model, parameters)\n",
    "\n",
    "            model.save_pretrained(f\"{save_path}/peft_{server_round}\")\n",
    "\n",
    "        return 0.0, {}\n",
    "\n",
    "    return evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T16:35:40.202724Z",
     "iopub.status.busy": "2025-10-26T16:35:40.202395Z",
     "iopub.status.idle": "2025-10-26T16:35:40.208921Z",
     "shell.execute_reply": "2025-10-26T16:35:40.208143Z",
     "shell.execute_reply.started": "2025-10-26T16:35:40.202699Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from flwr.server.strategy import (\n",
    "    DifferentialPrivacyClientSideFixedClipping\n",
    ")\n",
    "\n",
    "def server_fn(context: Context):\n",
    "    \"\"\"\n",
    "    This function returns a configured Flower server app, which will be passed to\n",
    "    start_simulation() later. It defines:\n",
    "     • The strategy for aggregation (FedAvg)\n",
    "     • How rounds behave (e.g., which clients to sample)\n",
    "     • Optional advanced features like Differential Privacy\n",
    "    \"\"\"\n",
    "    strategy = fl.server.strategy.FedAvg(\n",
    "        min_available_clients=cfg.flower.num_clients, # total clients\n",
    "        fraction_fit=cfg.flower.fraction_fit, # ratio of clients to sample\n",
    "        fraction_evaluate=0.0, # No federated evaluation\n",
    "        # A (optional) function used to configure a \"fit()\" round\n",
    "        on_fit_config_fn=get_on_fit_config(),\n",
    "        # A (optional) function to aggregate metrics sent by clients\n",
    "        fit_metrics_aggregation_fn=fit_weighted_average,\n",
    "        # A (optional) function to execute on the server after each round.\n",
    "        # In this example the function only saves the global model.\n",
    "        evaluate_fn=get_evaluate_fn(\n",
    "            cfg.model,\n",
    "            cfg.train.save_every_round,\n",
    "            cfg.flower.num_rounds,\n",
    "            save_path\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Add Differential Privacy to FedAvg strategy\n",
    "    sampled_clients = cfg.flower.num_clients*strategy.fraction_fit\n",
    "    strategy = DifferentialPrivacyClientSideFixedClipping(\n",
    "        strategy,\n",
    "        noise_multiplier=cfg.flower.dp.noise_mult,\n",
    "        clipping_norm=cfg.flower.dp.clip_norm,\n",
    "        num_sampled_clients=sampled_clients\n",
    "    )\n",
    "\n",
    "    # Number of rounds to run the simulation\n",
    "    num_rounds = cfg.flower.num_rounds\n",
    "    config = fl.server.ServerConfig(num_rounds=num_rounds)\n",
    "\n",
    "    return fl.server.ServerAppComponents(strategy=strategy, config=config)\n",
    "\n",
    "server = fl.server.ServerApp(server_fn=server_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T16:35:43.739849Z",
     "iopub.status.busy": "2025-10-26T16:35:43.739108Z",
     "iopub.status.idle": "2025-10-26T17:00:33.817484Z",
     "shell.execute_reply": "2025-10-26T17:00:33.816372Z",
     "shell.execute_reply.started": "2025-10-26T16:35:43.739824Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      Starting Flower ServerApp, config: num_rounds=2, no round_timeout\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [INIT]\n",
      "\u001b[92mINFO \u001b[0m:      Requesting initial parameters from one random client\n",
      "\u001b[92mINFO \u001b[0m:      Received initial parameters from one random client\n",
      "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n",
      "\u001b[92mINFO \u001b[0m:      initial parameters (loss, other metrics): 0.0, {}\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 4 clients (out of 20)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 4 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: central DP noise with 0.0025 stdev added\n",
      "\u001b[92mINFO \u001b[0m:      fit progress: (1, 0.0, {}, 721.141416849)\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: no clients selected, skipping evaluation\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 4 clients (out of 20)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 4 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: central DP noise with 0.0025 stdev added\n",
      "\u001b[92mINFO \u001b[0m:      fit progress: (2, 0.0, {}, 1455.4859777379997)\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: no clients selected, skipping evaluation\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [SUMMARY]\n",
      "\u001b[92mINFO \u001b[0m:      Run finished 2 round(s) in 1455.49s\n",
      "\u001b[92mINFO \u001b[0m:      \tHistory (loss, centralized):\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 0: 0.0\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 1: 0.0\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 2: 0.0\n",
      "\u001b[92mINFO \u001b[0m:      \tHistory (metrics, distributed, fit):\n",
      "\u001b[92mINFO \u001b[0m:      \t{'train_loss': [(1, 1.379931974887848), (2, 2.212392581819171)]}\n",
      "\u001b[92mINFO \u001b[0m:      \n"
     ]
    }
   ],
   "source": [
    "from logging import ERROR\n",
    "\n",
    "client_resources = dict(cfg.flower.client_resources)\n",
    "backend_setup = {\"logging_level\": ERROR, \"log_to_driver\": False}\n",
    "\n",
    "fl.simulation.run_simulation(\n",
    "    server_app=server,\n",
    "    client_app=client,\n",
    "    num_supernodes=cfg.flower.num_clients,\n",
    "    backend_config={\n",
    "        \"client_resources\": client_resources,\n",
    "        \"init_args\": backend_setup\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T17:01:28.512745Z",
     "iopub.status.busy": "2025-10-26T17:01:28.512400Z",
     "iopub.status.idle": "2025-10-26T17:01:47.755711Z",
     "shell.execute_reply": "2025-10-26T17:01:47.754846Z",
     "shell.execute_reply.started": "2025-10-26T17:01:28.512720Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6242e9d257b43aa867c7d8ed6d64eee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "861aaa67b2a143a78d8c13929e7dc7c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/tuhanasinan/llama32_1b_federated_finetuned_250steps/commit/c99a5874c8c69e0ebfcdb3a654248c8b6b2f5abe', commit_message='Upload LlamaForCausalLM', commit_description='', oid='c99a5874c8c69e0ebfcdb3a654248c8b6b2f5abe', pr_url=None, repo_url=RepoUrl('https://huggingface.co/tuhanasinan/llama32_1b_federated_finetuned_250steps', endpoint='https://huggingface.co', repo_type='model', repo_id='tuhanasinan/llama32_1b_federated_finetuned_250steps'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# a. Load the base 4-bit LLM\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# b. Wrap it with your federated fine-tuned adapter\n",
    "peft_model = PeftModel.from_pretrained(\n",
    "    base,\n",
    "    os.path.join(save_path, \"peft_2\"),    # path to your adapter folder\n",
    ")\n",
    "\n",
    "# c. Merge the base model with the adapter\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "\n",
    "# d. Push the merged model to HuggingFace Hub\n",
    "merged_model.push_to_hub(\"tuhanasinan/llama32_1b_federated_finetuned_250steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T17:04:07.278974Z",
     "iopub.status.busy": "2025-10-26T17:04:07.278348Z",
     "iopub.status.idle": "2025-10-26T17:04:22.387126Z",
     "shell.execute_reply": "2025-10-26T17:04:22.386131Z",
     "shell.execute_reply.started": "2025-10-26T17:04:07.278948Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama_index\n",
      "  Downloading llama_index-0.14.6-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting llama-index-cli<0.6,>=0.5.0 (from llama_index)\n",
      "  Downloading llama_index_cli-0.5.3-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting llama-index-core<0.15.0,>=0.14.6 (from llama_index)\n",
      "  Downloading llama_index_core-0.14.6-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting llama-index-embeddings-openai<0.6,>=0.5.0 (from llama_index)\n",
      "  Downloading llama_index_embeddings_openai-0.5.1-py3-none-any.whl.metadata (400 bytes)\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama_index)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.9.4-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting llama-index-llms-openai<0.7,>=0.6.0 (from llama_index)\n",
      "  Downloading llama_index_llms_openai-0.6.5-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting llama-index-readers-file<0.6,>=0.5.0 (from llama_index)\n",
      "  Downloading llama_index_readers_file-0.5.4-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama_index)\n",
      "  Downloading llama_index_readers_llama_parse-0.5.1-py3-none-any.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama_index) (3.9.1)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama_index) (3.12.15)\n",
      "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama_index) (0.21.0)\n",
      "Collecting banks<3,>=2.2.0 (from llama-index-core<0.15.0,>=0.14.6->llama_index)\n",
      "  Downloading banks-2.2.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama_index) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama_index) (1.2.18)\n",
      "Collecting dirtyjson<2,>=1.0.8 (from llama-index-core<0.15.0,>=0.14.6->llama_index)\n",
      "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama_index) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama_index) (2024.9.0)\n",
      "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama_index) (0.28.1)\n",
      "Collecting llama-index-workflows<3,>=2 (from llama-index-core<0.15.0,>=0.14.6->llama_index)\n",
      "  Downloading llama_index_workflows-2.8.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama_index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama_index) (3.5)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama_index) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama_index) (11.3.0)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama_index) (4.4.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama_index) (2.12.0a1)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama_index) (6.0.3)\n",
      "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama_index) (2.32.5)\n",
      "Collecting setuptools>=80.9.0 (from llama-index-core<0.15.0,>=0.14.6->llama_index)\n",
      "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15.0,>=0.14.6->llama_index) (2.0.41)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama_index) (8.5.0)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama_index) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama_index) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama_index) (4.15.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama_index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.15.0,>=0.14.6->llama_index) (1.17.2)\n",
      "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-embeddings-openai<0.6,>=0.5.0->llama_index) (1.97.1)\n",
      "Collecting llama-cloud==0.1.35 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama_index)\n",
      "  Downloading llama_cloud-0.1.35-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.11/dist-packages (from llama-cloud==0.1.35->llama-index-indices-managed-llama-cloud>=0.4.0->llama_index) (2025.8.3)\n",
      "Collecting openai>=1.1.0 (from llama-index-embeddings-openai<0.6,>=0.5.0->llama_index)\n",
      "  Downloading openai-1.109.1-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama_index) (4.13.4)\n",
      "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama_index) (0.7.1)\n",
      "Requirement already satisfied: pandas<2.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama_index) (2.2.3)\n",
      "Requirement already satisfied: pypdf<7,>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama_index) (6.1.0)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.6,>=0.5.0->llama_index)\n",
      "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_parse-0.6.76-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama_index) (8.1.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama_index) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama_index) (2025.9.18)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.6->llama_index) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.6->llama_index) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.6->llama_index) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.6->llama_index) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.6->llama_index) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.6->llama_index) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.6->llama_index) (1.20.1)\n",
      "Collecting griffe (from banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.6->llama_index)\n",
      "  Downloading griffe-1.14.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.6->llama_index) (3.1.6)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file<0.6,>=0.5.0->llama_index) (2.7)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.15.0,>=0.14.6->llama_index) (4.11.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.15.0,>=0.14.6->llama_index) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.15.0,>=0.14.6->llama_index) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.15.0,>=0.14.6->llama_index) (0.16.0)\n",
      "Collecting llama-index-instrumentation>=0.1.0 (from llama-index-workflows<3,>=2->llama-index-core<0.15.0,>=0.14.6->llama_index)\n",
      "  Downloading llama_index_instrumentation-0.4.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting llama-cloud-services>=0.6.76 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_cloud_services-0.6.76-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama_index) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama_index) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama_index) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama_index) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama_index) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama_index) (2025.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.15.0,>=0.14.6->llama_index) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.15.0,>=0.14.6->llama_index) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.15.0,>=0.14.6->llama_index) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.15.0,>=0.14.6->llama_index) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.15.0,>=0.14.6->llama_index) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->llama-index-core<0.15.0,>=0.14.6->llama_index) (2.4.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15.0,>=0.14.6->llama_index) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.37.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15.0,>=0.14.6->llama_index) (2.37.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15.0,>=0.14.6->llama_index) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.15.0,>=0.14.6->llama_index) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.15.0,>=0.14.6->llama_index) (2.5.0)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15.0,>=0.14.6->llama_index) (3.2.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.15.0,>=0.14.6->llama_index) (1.1.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.15.0,>=0.14.6->llama_index) (3.26.1)\n",
      "INFO: pip is looking at multiple versions of llama-cloud-services to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_parse-0.6.75-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.75 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_cloud_services-0.6.75-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_parse-0.6.74-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.74 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_cloud_services-0.6.74-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_parse-0.6.73-py3-none-any.whl.metadata (6.6 kB)\n",
      "INFO: pip is still looking at multiple versions of llama-cloud-services to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting llama-cloud-services>=0.6.73 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_cloud_services-0.6.73-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_parse-0.6.72-py3-none-any.whl.metadata (6.6 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting llama-cloud-services>=0.6.72 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_cloud_services-0.6.72-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_parse-0.6.71-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.71 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_cloud_services-0.6.71-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_parse-0.6.70-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.70 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_cloud_services-0.6.70-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_parse-0.6.69-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.69 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_cloud_services-0.6.69-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_parse-0.6.68-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.68 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_cloud_services-0.6.68-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_parse-0.6.67-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.67 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_cloud_services-0.6.67-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_parse-0.6.66-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.66 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_cloud_services-0.6.66-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_parse-0.6.65-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.64 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_cloud_services-0.6.65-py3-none-any.whl.metadata (3.3 kB)\n",
      "  Downloading llama_cloud_services-0.6.64-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_parse-0.6.64-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Downloading llama_parse-0.6.63-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.63 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_cloud_services-0.6.63-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_parse-0.6.62-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.62 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_cloud_services-0.6.62-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_parse-0.6.60-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.60 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_cloud_services-0.6.60-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_parse-0.6.59-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.59 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_cloud_services-0.6.59-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_parse-0.6.58-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.58 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_cloud_services-0.6.58-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_parse-0.6.57-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.56 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_cloud_services-0.6.57-py3-none-any.whl.metadata (3.7 kB)\n",
      "  Downloading llama_cloud_services-0.6.56-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_parse-0.6.56-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Downloading llama_parse-0.6.55-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.55 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_cloud_services-0.6.55-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_parse-0.6.54-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting llama-cloud-services>=0.6.54 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
      "  Downloading llama_cloud_services-0.6.54-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: python-dotenv<2,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-cloud-services>=0.6.54->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index) (1.1.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.15.0,>=0.14.6->llama_index) (25.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama_index) (1.17.0)\n",
      "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.11/dist-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.6->llama_index) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.6->llama_index) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->llama-index-core<0.15.0,>=0.14.6->llama_index) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->llama-index-core<0.15.0,>=0.14.6->llama_index) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->llama-index-core<0.15.0,>=0.14.6->llama_index) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->llama-index-core<0.15.0,>=0.14.6->llama_index) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->llama-index-core<0.15.0,>=0.14.6->llama_index) (2024.2.0)\n",
      "Downloading llama_index-0.14.6-py3-none-any.whl (7.4 kB)\n",
      "Downloading llama_index_cli-0.5.3-py3-none-any.whl (28 kB)\n",
      "Downloading llama_index_core-0.14.6-py3-none-any.whl (11.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_embeddings_openai-0.5.1-py3-none-any.whl (7.0 kB)\n",
      "Downloading llama_index_indices_managed_llama_cloud-0.9.4-py3-none-any.whl (17 kB)\n",
      "Downloading llama_cloud-0.1.35-py3-none-any.whl (303 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_llms_openai-0.6.5-py3-none-any.whl (26 kB)\n",
      "Downloading llama_index_readers_file-0.5.4-py3-none-any.whl (51 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_readers_llama_parse-0.5.1-py3-none-any.whl (3.2 kB)\n",
      "Downloading banks-2.2.0-py3-none-any.whl (29 kB)\n",
      "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Downloading llama_index_workflows-2.8.3-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llama_parse-0.6.54-py3-none-any.whl (4.9 kB)\n",
      "Downloading llama_cloud_services-0.6.54-py3-none-any.whl (63 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.109.1-py3-none-any.whl (948 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m948.6/948.6 kB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Downloading llama_index_instrumentation-0.4.2-py3-none-any.whl (15 kB)\n",
      "Downloading griffe-1.14.0-py3-none-any.whl (144 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.4/144.4 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: striprtf, dirtyjson, setuptools, griffe, openai, llama-index-instrumentation, llama-cloud, banks, llama-index-workflows, llama-index-core, llama-cloud-services, llama-parse, llama-index-llms-openai, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-readers-file, llama-index-indices-managed-llama-cloud, llama-index-cli, llama_index\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 75.2.0\n",
      "    Uninstalling setuptools-75.2.0:\n",
      "      Successfully uninstalled setuptools-75.2.0\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.97.1\n",
      "    Uninstalling openai-1.97.1:\n",
      "      Successfully uninstalled openai-1.97.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "pandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed banks-2.2.0 dirtyjson-1.0.8 griffe-1.14.0 llama-cloud-0.1.35 llama-cloud-services-0.6.54 llama-index-cli-0.5.3 llama-index-core-0.14.6 llama-index-embeddings-openai-0.5.1 llama-index-indices-managed-llama-cloud-0.9.4 llama-index-instrumentation-0.4.2 llama-index-llms-openai-0.6.5 llama-index-readers-file-0.5.4 llama-index-readers-llama-parse-0.5.1 llama-index-workflows-2.8.3 llama-parse-0.6.54 llama_index-0.14.6 openai-1.109.1 setuptools-80.9.0 striprtf-0.0.26\n"
     ]
    }
   ],
   "source": [
    "!pip install llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T17:04:57.499622Z",
     "iopub.status.busy": "2025-10-26T17:04:57.498907Z",
     "iopub.status.idle": "2025-10-26T17:04:57.555808Z",
     "shell.execute_reply": "2025-10-26T17:04:57.554829Z",
     "shell.execute_reply.started": "2025-10-26T17:04:57.499599Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llama_index.llms.huggingface'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_235/2096498282.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhuggingface\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHuggingFaceLLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMarkdown\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m llm = HuggingFaceLLM(\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llama_index.llms.huggingface'"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    model_name=\"tuhanasinan/llama32_1b_federated_finetuned_250steps\",\n",
    "    tokenizer_name=\"meta-llama/Llama-3.2-1B-Instruct\" \n",
    ")\n",
    "prompt = \"What are the management strategies for antiphospholipid syndrome?\"\n",
    "response = llm.complete(prompt)\n",
    "\n",
    "display(Markdown(str(response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T17:05:58.329061Z",
     "iopub.status.busy": "2025-10-26T17:05:58.328365Z",
     "iopub.status.idle": "2025-10-26T17:06:18.864361Z",
     "shell.execute_reply": "2025-10-26T17:06:18.863415Z",
     "shell.execute_reply.started": "2025-10-26T17:05:58.329032Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5904cae467c40b6ab4c9c66e7f3dc3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d98fcb9f1fe040b19c0f939b0b6b0fd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.07G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5adaaf7e13f44f3dafb040a3b3d4fd8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "What are the management strategies for antiphospholipid syndrome? \n",
       "Management strategies for antiphospholipid syndrome (APS) include:\n",
       "1. Anticoagulation therapy: The most common treatment for APS is anticoagulation therapy with specific anticoagulant medications, such as low molecular weight heparin or fondaparick.\n",
       "2. Low molecular weight heparin therapy: Low molecular weight heparin is often used as a first-line treatment for APS.\n",
       "3. Fibrinolytic therapy: Fibrinolytic therapy, such as recombinant activated protein C (rAPC) or activated protein C (APC), may be used in some cases of APS.\n",
       "4. Corticosteroid therapy: Corticosteroid therapy may be used to treat APS, especially in cases where anticoagulation therapy is not effective.\n",
       "5. Vasculature-based therapy: Vasculature-based therapy, such as carotid endarterectomy or carotid artery bypass grafting, may be used to address arterial complications associated with APS.\n",
       "6. Fetal-maternal transfusion: Fetal-maternal transfusion may be used to prevent complications associated with APS, such as placental abruption.\n",
       "7. Lifestyle modifications: Lifestyle modifications, such as"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Model ve tokenizer adlarını belirle\n",
    "model_name = \"tuhanasinan/llama32_1b_federated_finetuned_250steps\"\n",
    "tokenizer_name = \"meta-llama/Llama-3.2-1B-Instruct\" \n",
    "\n",
    "# Tokenizer'ı yükle\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "# Modeli yükle (GPU varsa otomatik olarak GPU'ya yükler)\n",
    "# torch_dtype=torch.bfloat16 performansı artırabilir\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    device_map=\"auto\" # Modeli uygun cihazlara (GPU/CPU) dağıtır\n",
    ")\n",
    "\n",
    "# Metin üretimi için bir pipeline oluştur\n",
    "text_generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Prompt'u tanımla ve metni üret\n",
    "prompt = \"What are the management strategies for antiphospholipid syndrome?\"\n",
    "# Pipeline'a ek parametreler verebilirsin (max_new_tokens vb.)\n",
    "sequences = text_generator(\n",
    "    prompt,\n",
    "    max_new_tokens=250,  # Üretilecek maksimum yeni token sayısı\n",
    "    do_sample=True,      # Örnekleme yaparak daha çeşitli sonuçlar al\n",
    "    temperature=0.7,     # Daha yaratıcı (yüksek) veya deterministik (düşük) sonuçlar için\n",
    "    top_k=50,            # En olası K token arasından seçim yap\n",
    "    top_p=0.95,          # Kümülatif olasılığı P olan tokenlar arasından seçim yap\n",
    "    num_return_sequences=1, # Kaç tane sonuç döndürüleceği\n",
    ")\n",
    "\n",
    "# Sonucu göster\n",
    "# Pipeline genellikle [{'generated_text': '...'}] formatında bir liste döndürür\n",
    "if sequences and len(sequences) > 0:\n",
    "    response_text = sequences[0]['generated_text']\n",
    "    display(Markdown(response_text))\n",
    "else:\n",
    "    print(\"Model yanıt üretemedi.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
